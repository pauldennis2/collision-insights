{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12f7755",
   "metadata": {},
   "source": [
    "# The Museum of Ideas that Didn't Quite Work Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output here was a mix of intuitive and counter-intuitive\n",
    "\n",
    "print(\"--- Heatmap: Lift of Accident Severity by Consolidated Weather Condition ---\")\n",
    "\n",
    "# 1. Calculate overall prevalence of each Severity level\n",
    "global_severity_counts_spark = df_categorized_weather.groupBy(\"Severity\").count()\n",
    "global_severity_counts_pd = global_severity_counts_spark.toPandas()\n",
    "total_accidents = global_severity_counts_pd['count'].sum()\n",
    "global_severity_proportions = global_severity_counts_pd.set_index('Severity')['count'] / total_accidents\n",
    "\n",
    "print(\"\\nGlobal Severity Proportions:\")\n",
    "print(global_severity_proportions)\n",
    "\n",
    "# 2. Calculate counts for each Consolidated_Weather and Severity combination (as before)\n",
    "severity_weather_counts = df_categorized_weather.groupBy(\"Consolidated_Weather\", \"Severity\").count().orderBy(\"Consolidated_Weather\", \"Severity\")\n",
    "severity_weather_pd = severity_weather_counts.toPandas()\n",
    "\n",
    "# 3. Pivot the DataFrame to get weather conditions as index and severity as columns\n",
    "pivot_table = severity_weather_pd.pivot_table(\n",
    "    index='Consolidated_Weather',\n",
    "    columns='Severity',\n",
    "    values='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# 4. Calculate P(Severity | Weather_Condition) - proportions within each weather condition\n",
    "pivot_table_conditional_prob = pivot_table.div(pivot_table.sum(axis=1), axis=0) # Already in decimal, not percentage for lift calc\n",
    "\n",
    "# 5. Calculate Lift Score for each cell: P(Severity | Weather) / P(Severity)\n",
    "lift_table = pivot_table_conditional_prob.copy()\n",
    "for severity_level in global_severity_proportions.index:\n",
    "    if severity_level in lift_table.columns and global_severity_proportions[severity_level] > 0:\n",
    "        lift_table[severity_level] = lift_table[severity_level] / global_severity_proportions[severity_level]\n",
    "    else:\n",
    "        # Handle cases where global proportion is 0 (shouldn't happen for 1-4) or column missing\n",
    "        lift_table[severity_level] = 0 # Or handle as NaN if preferred\n",
    "\n",
    "# Ensure severity columns are ordered correctly (1, 2, 3, 4) for consistent plotting\n",
    "ordered_columns = [1, 2, 3, 4]\n",
    "if all(col_name in lift_table.columns for col_name in ordered_columns):\n",
    "    lift_table = lift_table[ordered_columns]\n",
    "else:\n",
    "    # Handle case where some severity levels might be missing (e.g., if a weather condition never had Sev1)\n",
    "    # This might happen if original data is sparse or small. For our large dataset, unlikely.\n",
    "    # If it happens, we'd fill missing columns with 0.\n",
    "    for col_name in ordered_columns:\n",
    "        if col_name not in lift_table.columns:\n",
    "            lift_table[col_name] = 0\n",
    "    lift_table = lift_table[ordered_columns] # Re-order\n",
    "\n",
    "\n",
    "# Sort the index (Consolidated Weather Conditions) by the lift of higher severities for better insights\n",
    "# We can sort by the lift of Severity 4, then Severity 3, to highlight the most \"dangerous\" conditions\n",
    "lift_table_sorted = lift_table.sort_values(by=[4, 3], ascending=[False, False])\n",
    "\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8)) # Adjust size as needed for 17 rows x 4 columns\n",
    "sns.heatmap(\n",
    "    lift_table_sorted,\n",
    "    annot=True,     # Show the lift values on the heatmap\n",
    "    fmt=\".1f\",      # Format annotations to one decimal place\n",
    "    cmap=\"viridis\",  # Color map (often good for quantitative scales, \"YlGnBu\" or \"OrRd\" also work)\n",
    "    linewidths=.5,  # Add lines between cells\n",
    "    cbar_kws={'label': 'Lift Score (Relative Risk)'} # Color bar label\n",
    ")\n",
    "\n",
    "plt.title('Lift of Accident Severity by Consolidated Weather Condition')\n",
    "plt.xlabel('Accident Severity')\n",
    "plt.ylabel('Consolidated Weather Condition')\n",
    "plt.yticks(rotation=0) # Keep Y-labels horizontal\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLift Table (sorted by Severity 4, then Severity 3 lift):\")\n",
    "print(lift_table_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6529bb6",
   "metadata": {},
   "source": [
    "### Roundabout Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We looked for examples of \"Before and afters\" on roundabouts and found... 0\n",
    "\n",
    "print(\"Original Start_Lat Dtype:\", df.schema[\"Start_Lat\"].dataType)\n",
    "print(\"Original Start_Lng Dtype:\", df.schema[\"Start_Lng\"].dataType)\n",
    "\n",
    "# Using .cast() directly is the most straightforward for numeric conversion in PySpark\n",
    "df = df.withColumn(\"Start_Lat\", col(\"Start_Lat\").cast(DoubleType()))\n",
    "df = df.withColumn(\"Start_Lng\", col(\"Start_Lng\").cast(DoubleType()))\n",
    "\n",
    "print(\"\\nAfter conversion (if needed):\")\n",
    "print(\"New Start_Lat Dtype:\", df.schema[\"Start_Lat\"].dataType)\n",
    "print(\"New Start_Lng Dtype:\", df.schema[\"Start_Lng\"].dataType)\n",
    "\n",
    "# Assess Decimal Place Precision \n",
    "\n",
    "# Get the string representation of the numbers, replace 'null' or 'NaN' with empty string\n",
    "df = df.withColumn(\"Start_Lat_str\", regexp_replace(col(\"Start_Lat\").cast(StringType()), \"null\", \"\"))\n",
    "df = df.withColumn(\"Start_Lng_str\", regexp_replace(col(\"Start_Lng\").cast(StringType()), \"null\", \"\"))\n",
    "\n",
    "# Calculate decimal places:\n",
    "# If there's a '.', calculate length after it. Else, 0.\n",
    "df = df.withColumn(\n",
    "    \"Start_Lat_decimals\",\n",
    "    when(instr(col(\"Start_Lat_str\"), \".\") > 0, length(col(\"Start_Lat_str\")) - instr(col(\"Start_Lat_str\"), \".\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Start_Lng_decimals\",\n",
    "    when(instr(col(\"Start_Lng_str\"), \".\") > 0, length(col(\"Start_Lng_str\")) - instr(col(\"Start_Lng_str\"), \".\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Get the distribution of decimal places\n",
    "print(\"\\nStart_Lat Decimal Place Distribution (All Records):\")\n",
    "df.groupBy(\"Start_Lat_decimals\").count().orderBy(\"Start_Lat_decimals\").show(truncate=False)\n",
    "\n",
    "print(\"\\nStart_Lng Decimal Place Distribution (All Records):\")\n",
    "df.groupBy(\"Start_Lng_decimals\").count().orderBy(\"Start_Lng_decimals\").show(truncate=False)\n",
    "\n",
    "# Remove the temporary string and decimal columns if you wish\n",
    "df = df.drop(\"Start_Lat_str\", \"Start_Lng_str\", \"Start_Lat_decimals\", \"Start_Lng_decimals\")\n",
    "\n",
    "print(\"\\nStart_Lat Descriptive Statistics:\")\n",
    "df.select(col(\"Start_Lat\")).describe().show()\n",
    "\n",
    "print(\"\\nStart_Lng Descriptive Statistics:\")\n",
    "df.select(col(\"Start_Lng\")).describe().show()\n",
    "\n",
    "# Check for null values\n",
    "print(f\"\\nNull Start_Lat values: {df.filter(col('Start_Lat').isNull()).count()}\")\n",
    "print(f\"Null Start_Lng values: {df.filter(col('Start_Lng').isNull()).count()}\")\n",
    "\n",
    "# Check for values that fall outside typical US bounds (e.g., beyond 24-50 Lat, -66 to -125 Lng)\n",
    "lat_outliers = df.filter((col(\"Start_Lat\") < 24) | (col(\"Start_Lat\") > 50)).count()\n",
    "lng_outliers = df.filter((col(\"Start_Lng\") < -125) | (col(\"Start_Lng\") > -66)).count()\n",
    "\n",
    "print(f\"Start_Lat outliers (outside 24-50): {lat_outliers}\")\n",
    "print(f\"Start_Lng outliers (outside -125 to -66): {lng_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Roundabout').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84646301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Schema of DataFrame 'df' ---\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n--- Distribution of 'Roundabout' column ---\")\n",
    "df.groupBy(\"Roundabout\").count().show()\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Start_Lat\", col(\"Start_Lat\").cast(DoubleType()))\n",
    "df = df.withColumn(\"Start_Lng\", col(\"Start_Lng\").cast(DoubleType()))\n",
    "\n",
    "# Filter out rows where Start_Lat or Start_Lng are null after casting\n",
    "df_filtered_coords = df.filter(col(\"Start_Lat\").isNotNull() & col(\"Start_Lng\").isNotNull())\n",
    "\n",
    "# Define the precision for grouping nearby locations.\n",
    "ROUNDING_PRECISION = 4\n",
    "\n",
    "# 1. Create a \"location_group_id\" by rounding lat/long to group nearby accidents.\n",
    "df_with_location_key = df_filtered_coords.withColumn(\"rounded_lat\", round(col(\"Start_Lat\"), ROUNDING_PRECISION)) \\\n",
    "                                         .withColumn(\"rounded_lng\", round(col(\"Start_Lng\"), ROUNDING_PRECISION))\n",
    "\n",
    "# --- Debugging the potential agg issue ---\n",
    "print(\"\\n--- Schema of DataFrame 'df_with_location_key' before agg ---\")\n",
    "df_with_location_key.printSchema()\n",
    "\n",
    "# 2. Identify unique rounded locations that have *both*\n",
    "#    accidents where 'Roundabout' is TRUE AND accidents where 'Roundabout' is FALSE.\n",
    "#    This is our robust indicator of a potentially \"changed\" intersection.\n",
    "potential_changed_locations = df_with_location_key.groupBy(\"rounded_lat\", \"rounded_lng\") \\\n",
    "    .agg(\n",
    "        # Count accidents where 'Roundabout' is true\n",
    "        spark_sum(when(col(\"Roundabout\") == True, 1).otherwise(0)).alias(\"roundabout_accident_count\"),\n",
    "        # Count accidents where 'Roundabout' is false (meaning some other control type was present)\n",
    "        spark_sum(when(col(\"Roundabout\") == False, 1).otherwise(0)).alias(\"non_roundabout_accident_count\")\n",
    "    ) \\\n",
    "    .filter((col(\"roundabout_accident_count\") > 0) & (col(\"non_roundabout_accident_count\") > 0))\n",
    "\n",
    "# 3. Join back to the DataFrame to get all accidents associated with these identified locations\n",
    "changed_intersections_accidents = df_with_location_key.join(\n",
    "    potential_changed_locations.select(\"rounded_lat\", \"rounded_lng\"), # Select only the join keys\n",
    "    on=[\"rounded_lat\", \"rounded_lng\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 4. Count the total number of accidents at these potentially \"changed\" intersections\n",
    "total_accidents_at_changed_intersections = changed_intersections_accidents.count()\n",
    "\n",
    "print(f\"\\nTotal count of accidents at potentially 'changed' intersections (based on {ROUNDING_PRECISION} decimal rounding for location matching): {total_accidents_at_changed_intersections}\")\n",
    "\n",
    "num_unique_changed_locations = potential_changed_locations.count()\n",
    "print(f\"Number of unique potentially 'changed' intersection locations identified: {num_unique_changed_locations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d288c51",
   "metadata": {},
   "source": [
    "## Description Investigation - Trucks, Intoxication\n",
    "\n",
    "We looked at the description field and found it was extremely limited. The \"Descriptions\" are more like radio announcements about which lanes are closed; no real accident info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6520f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drunk_words = \"drunk|intoxicated|sobriety|alcohol|DUI\"\n",
    "\n",
    "drunk_intoxicated_accidents_count = df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(f\"(?i){drunk_words}\"))\n",
    ").count()\n",
    "\n",
    "print(f\"\\nTotal count of accidents involving {drunk_words} in the description: {drunk_intoxicated_accidents_count}\")\n",
    "\n",
    "print(f\"\\nSample of descriptions involving {drunk_words} (first 10):\")\n",
    "df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(f\"(?i){drunk_words}\"))\n",
    ").select(\"Description\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe43747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some lazy person didn't change their variable names, tsk tsk\n",
    "\n",
    "drunk_intoxicated_accidents_count = df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(\"(?i)truck\"))\n",
    ").count()\n",
    "\n",
    "print(f\"\\nTotal count of accidents involving trucks in the description: {drunk_intoxicated_accidents_count}\")\n",
    "\n",
    "print(\"\\nSample of descriptions involving trucks (first 10):\")\n",
    "df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(\"(?i)truck\"))\n",
    ").select(\"Description\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca3b22",
   "metadata": {},
   "source": [
    "## DST Investigation\n",
    "\n",
    "Methods may be wrong as we found nothing close to statistically significant increase in accidents during periods after DST compared to before. (Some years even included a substantial drop in accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Date\", F.to_date(\"Start_Time\"))\n",
    "\n",
    "# Hardcoded DST start dates for each year\n",
    "dst_dates = {\n",
    "    2016: \"2016-03-13\",\n",
    "    2017: \"2017-03-12\",\n",
    "    2018: \"2018-03-11\",\n",
    "    2019: \"2019-03-10\",\n",
    "    2020: \"2020-03-08\",\n",
    "    2021: \"2021-03-14\",\n",
    "    2022: \"2022-03-13\",\n",
    "    2023: \"2023-03-12\",\n",
    "}\n",
    "\n",
    "# Collect results for each year\n",
    "results = []\n",
    "for year, dst_date in sorted(dst_dates.items()):  # Ensure correct year order\n",
    "    before_dst = df.filter(F.col(\"Date\").between(F.lit(dst_date) - F.expr(\"INTERVAL 6 DAY\"), F.lit(dst_date) - F.expr(\"INTERVAL 4 DAY\")))\n",
    "    after_dst = df.filter(F.col(\"Date\").between(F.lit(dst_date) + F.expr(\"INTERVAL 1 DAY\"), F.lit(dst_date) + F.expr(\"INTERVAL 3 DAY\")))\n",
    "\n",
    "    before_count = before_dst.count()\n",
    "    after_count = after_dst.count()\n",
    "\n",
    "    results.append((year, before_count, after_count))\n",
    "\n",
    "# Print results in ascending order\n",
    "for year, before, after in results:\n",
    "    print(f\"{year}: Accidents Monday-Wednesday BEFORE DST: {before}, AFTER DST: {after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506654ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on intervals\n",
    "\n",
    "# Example DataFrame with one test date\n",
    "test_df = spark.createDataFrame([(datetime(2023, 3, 12),)], [\"TestDate\"]).withColumn(\"TestDate\", F.col(\"TestDate\").cast(DateType()))\n",
    "\n",
    "# Applying INTERVAL shifts\n",
    "test_df = test_df.withColumn(\"Minus_6_Days\", F.col(\"TestDate\") - F.expr(\"INTERVAL 6 DAY\"))\n",
    "test_df = test_df.withColumn(\"Minus_4_Days\", F.col(\"TestDate\") - F.expr(\"INTERVAL 4 DAY\"))\n",
    "test_df = test_df.withColumn(\"Plus_1_Day\", F.col(\"TestDate\") + F.expr(\"INTERVAL 1 DAY\"))\n",
    "test_df = test_df.withColumn(\"Plus_3_Days\", F.col(\"TestDate\") + F.expr(\"INTERVAL 3 DAY\"))\n",
    "\n",
    "# Show results\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No one likes Mondays!\n",
    "\n",
    "df = df.withColumn(\"Date\", F.to_date(\"Start_Time\"))\n",
    "\n",
    "# Hardcoded Daylight Savings time dates (checked by calendar sampling)\n",
    "dst_dates = {\n",
    "    2016: \"2016-03-13\",\n",
    "    2017: \"2017-03-12\",\n",
    "    2018: \"2018-03-11\",\n",
    "    2019: \"2019-03-10\",\n",
    "    2020: \"2020-03-08\",\n",
    "    2021: \"2021-03-14\",\n",
    "    2022: \"2022-03-13\",\n",
    "    2023: \"2023-03-12\",\n",
    "}\n",
    "\n",
    "# Collect results for each year\n",
    "results = []\n",
    "for year, dst_date in sorted(dst_dates.items()):\n",
    "    monday_before = df.filter(F.col(\"Date\") == F.lit(dst_date) - F.expr(\"INTERVAL 6 DAY\"))\n",
    "    monday_after = df.filter(F.col(\"Date\") == F.lit(dst_date) + F.expr(\"INTERVAL 1 DAY\"))\n",
    "\n",
    "    before_count = monday_before.count()\n",
    "    after_count = monday_after.count()\n",
    "\n",
    "    results.append((year, before_count, after_count))\n",
    "\n",
    "# Print results in ascending order\n",
    "for year, before, after in results:\n",
    "    print(f\"{year}: Accidents on Monday BEFORE DST: {before}, AFTER DST: {after}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82efdc2",
   "metadata": {},
   "source": [
    "## Lighting intermed steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c663ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_highways = [\"I-95\", \"I-5\", \"I-10\"] # quick sample\n",
    "\n",
    "# Filter dataset for selected highways\n",
    "df_selected = df.filter(F.col(\"Street_minus_dir\").isin(selected_highways))\n",
    "\n",
    "# Compute county-level severity metrics\n",
    "county_severity = (\n",
    "    df_selected.groupBy(\"Street_minus_dir\", \"State\", \"County\")\n",
    "    .agg(\n",
    "        F.sum(F.when(F.col(\"Sunrise_Sunset\") == \"Night\", F.col(\"Severity_Score\")).otherwise(0)).alias(\"Night_Severity\"),\n",
    "        F.sum(\"Severity_Score\").alias(\"Total_Severity\"),\n",
    "        F.count(F.when(F.col(\"Sunrise_Sunset\") == \"Night\", True)).alias(\"Night_Accident_Count\")\n",
    "    )\n",
    "    .withColumn(\"Night_Severity_Ratio\", F.col(\"Night_Severity\") / F.col(\"Total_Severity\"))\n",
    ")\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"created_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export separate CSVs for each highway\n",
    "for highway in selected_highways:\n",
    "    highway_df = county_severity.filter(F.col(\"Street_minus_dir\") == highway).toPandas()\n",
    "    highway_df.to_csv(f\"{output_dir}/{highway}_severity.csv\", index=False)\n",
    "\n",
    "print(\"CSV exports completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeda467c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Define selected highways\n",
    "selected_highways = [\"I-95\", \"I-5\", \"I-10\"]\n",
    "\n",
    "# Filter dataset for selected highways\n",
    "df_selected = df.filter(F.col(\"Street_minus_dir\").isin(selected_highways))\n",
    "\n",
    "# Extract accident locations for mapping\n",
    "accident_locations = (\n",
    "    df_selected.select(\"Street_minus_dir\", \"State\", \"County\", \"Severity\", \"Start_Lat\", \"Start_Lng\")\n",
    "    .filter(F.col(\"Start_Lat\").isNotNull() & F.col(\"Start_Lng\").isNotNull())\n",
    ")\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"created_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export separate CSVs for each highway\n",
    "for highway in selected_highways:\n",
    "    highway_df = accident_locations.filter(F.col(\"Street_minus_dir\") == highway).toPandas()\n",
    "    highway_df.to_csv(f\"{output_dir}/{highway}_accidents.csv\", index=False)\n",
    "\n",
    "print(\"CSV exports completed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad228af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"That belongs in a museum!\"\n",
    "# Define selected highways\n",
    "selected_highway = \"I-5\"  # Modify as needed\n",
    "\n",
    "# Filter dataset for selected highway\n",
    "df_selected = df.filter(F.col(\"Street_minus_dir\") == selected_highway)\n",
    "\n",
    "# Round lat/lon values for uniqueness\n",
    "rounded_locations = (\n",
    "    df_selected.select(\n",
    "        F.round(F.col(\"Start_Lat\"), 2).alias(\"Latitude\"),\n",
    "        F.round(F.col(\"Start_Lng\"), 2).alias(\"Longitude\")\n",
    "    )\n",
    "    .distinct()  # Keep only unique lat/lon pairs\n",
    ")\n",
    "\n",
    "# Set output directory\n",
    "output_dir = \"created_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export to CSV\n",
    "rounded_locations.toPandas().to_csv(f\"{output_dir}/{selected_highway}_unique_locations.csv\", index=False)\n",
    "\n",
    "print(f\"CSV export completed for {selected_highway}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274d25cc",
   "metadata": {},
   "source": [
    "### Failed covid investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea4c277",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Analyzing Accident Frequency by Calendar Day ---\")\n",
    "\n",
    "df_doy_accidents = df.withColumn(\"Day_of_year\", dayofyear(col(\"Start_Time\")))\n",
    "daily_counts_spark = df_doy_accidents.groupBy(\"Day_of_year\").count().orderBy(\"Day_of_year\")\n",
    "\n",
    "daily_counts_pd = daily_counts_spark.toPandas()\n",
    "\n",
    "all_days = pd.DataFrame({'Day_of_year': range(365)})\n",
    "daily_counts_pd = pd.merge(all_days, daily_counts_pd, on='Day_of_year', how='left').fillna(0)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(daily_counts_pd['Day_of_year'], weights=daily_counts_pd['count'], bins=60, color='skyblue', edgecolor='black')\n",
    "\n",
    "plt.title('Number of Accidents by Day of Year')\n",
    "plt.xlabel('Day of Year, Starting Jan 1')\n",
    "plt.ylabel('Number of Accidents')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2208723b",
   "metadata": {},
   "source": [
    "## Clearance time investigation\n",
    "\n",
    "Somehow it took you 6 months to clear an accident? Full of data entry errors and unusable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc27cb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter rows where Clear_Time exceeds 24 hours\n",
    "outlier_df = df.filter(col(\"Clear_Time\") > 24).select(\"Start_Time\", \"End_Time\", \"Clear_Time\")\n",
    "\n",
    "# Show results\n",
    "outlier_df.show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collision_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
