{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12f7755",
   "metadata": {},
   "source": [
    "# The Museum of Ideas that Didn't Quite Work Out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The output here was a mix of intuitive and counter-intuitive\n",
    "\n",
    "print(\"--- Heatmap: Lift of Accident Severity by Consolidated Weather Condition ---\")\n",
    "\n",
    "# 1. Calculate overall prevalence of each Severity level\n",
    "global_severity_counts_spark = df_categorized_weather.groupBy(\"Severity\").count()\n",
    "global_severity_counts_pd = global_severity_counts_spark.toPandas()\n",
    "total_accidents = global_severity_counts_pd['count'].sum()\n",
    "global_severity_proportions = global_severity_counts_pd.set_index('Severity')['count'] / total_accidents\n",
    "\n",
    "print(\"\\nGlobal Severity Proportions:\")\n",
    "print(global_severity_proportions)\n",
    "\n",
    "# 2. Calculate counts for each Consolidated_Weather and Severity combination (as before)\n",
    "severity_weather_counts = df_categorized_weather.groupBy(\"Consolidated_Weather\", \"Severity\").count().orderBy(\"Consolidated_Weather\", \"Severity\")\n",
    "severity_weather_pd = severity_weather_counts.toPandas()\n",
    "\n",
    "# 3. Pivot the DataFrame to get weather conditions as index and severity as columns\n",
    "pivot_table = severity_weather_pd.pivot_table(\n",
    "    index='Consolidated_Weather',\n",
    "    columns='Severity',\n",
    "    values='count',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# 4. Calculate P(Severity | Weather_Condition) - proportions within each weather condition\n",
    "pivot_table_conditional_prob = pivot_table.div(pivot_table.sum(axis=1), axis=0) # Already in decimal, not percentage for lift calc\n",
    "\n",
    "# 5. Calculate Lift Score for each cell: P(Severity | Weather) / P(Severity)\n",
    "lift_table = pivot_table_conditional_prob.copy()\n",
    "for severity_level in global_severity_proportions.index:\n",
    "    if severity_level in lift_table.columns and global_severity_proportions[severity_level] > 0:\n",
    "        lift_table[severity_level] = lift_table[severity_level] / global_severity_proportions[severity_level]\n",
    "    else:\n",
    "        # Handle cases where global proportion is 0 (shouldn't happen for 1-4) or column missing\n",
    "        lift_table[severity_level] = 0 # Or handle as NaN if preferred\n",
    "\n",
    "# Ensure severity columns are ordered correctly (1, 2, 3, 4) for consistent plotting\n",
    "ordered_columns = [1, 2, 3, 4]\n",
    "if all(col_name in lift_table.columns for col_name in ordered_columns):\n",
    "    lift_table = lift_table[ordered_columns]\n",
    "else:\n",
    "    # Handle case where some severity levels might be missing (e.g., if a weather condition never had Sev1)\n",
    "    # This might happen if original data is sparse or small. For our large dataset, unlikely.\n",
    "    # If it happens, we'd fill missing columns with 0.\n",
    "    for col_name in ordered_columns:\n",
    "        if col_name not in lift_table.columns:\n",
    "            lift_table[col_name] = 0\n",
    "    lift_table = lift_table[ordered_columns] # Re-order\n",
    "\n",
    "\n",
    "# Sort the index (Consolidated Weather Conditions) by the lift of higher severities for better insights\n",
    "# We can sort by the lift of Severity 4, then Severity 3, to highlight the most \"dangerous\" conditions\n",
    "lift_table_sorted = lift_table.sort_values(by=[4, 3], ascending=[False, False])\n",
    "\n",
    "\n",
    "# Create the heatmap\n",
    "plt.figure(figsize=(10, 8)) # Adjust size as needed for 17 rows x 4 columns\n",
    "sns.heatmap(\n",
    "    lift_table_sorted,\n",
    "    annot=True,     # Show the lift values on the heatmap\n",
    "    fmt=\".1f\",      # Format annotations to one decimal place\n",
    "    cmap=\"viridis\",  # Color map (often good for quantitative scales, \"YlGnBu\" or \"OrRd\" also work)\n",
    "    linewidths=.5,  # Add lines between cells\n",
    "    cbar_kws={'label': 'Lift Score (Relative Risk)'} # Color bar label\n",
    ")\n",
    "\n",
    "plt.title('Lift of Accident Severity by Consolidated Weather Condition')\n",
    "plt.xlabel('Accident Severity')\n",
    "plt.ylabel('Consolidated Weather Condition')\n",
    "plt.yticks(rotation=0) # Keep Y-labels horizontal\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLift Table (sorted by Severity 4, then Severity 3 lift):\")\n",
    "print(lift_table_sorted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6529bb6",
   "metadata": {},
   "source": [
    "### Roundabout Investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00b6355",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We looked for examples of \"Before and afters\" on roundabouts and found... 0\n",
    "\n",
    "print(\"Original Start_Lat Dtype:\", df.schema[\"Start_Lat\"].dataType)\n",
    "print(\"Original Start_Lng Dtype:\", df.schema[\"Start_Lng\"].dataType)\n",
    "\n",
    "# Using .cast() directly is the most straightforward for numeric conversion in PySpark\n",
    "df = df.withColumn(\"Start_Lat\", col(\"Start_Lat\").cast(DoubleType()))\n",
    "df = df.withColumn(\"Start_Lng\", col(\"Start_Lng\").cast(DoubleType()))\n",
    "\n",
    "print(\"\\nAfter conversion (if needed):\")\n",
    "print(\"New Start_Lat Dtype:\", df.schema[\"Start_Lat\"].dataType)\n",
    "print(\"New Start_Lng Dtype:\", df.schema[\"Start_Lng\"].dataType)\n",
    "\n",
    "# Assess Decimal Place Precision \n",
    "\n",
    "# Get the string representation of the numbers, replace 'null' or 'NaN' with empty string\n",
    "df = df.withColumn(\"Start_Lat_str\", regexp_replace(col(\"Start_Lat\").cast(StringType()), \"null\", \"\"))\n",
    "df = df.withColumn(\"Start_Lng_str\", regexp_replace(col(\"Start_Lng\").cast(StringType()), \"null\", \"\"))\n",
    "\n",
    "# Calculate decimal places:\n",
    "# If there's a '.', calculate length after it. Else, 0.\n",
    "df = df.withColumn(\n",
    "    \"Start_Lat_decimals\",\n",
    "    when(instr(col(\"Start_Lat_str\"), \".\") > 0, length(col(\"Start_Lat_str\")) - instr(col(\"Start_Lat_str\"), \".\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"Start_Lng_decimals\",\n",
    "    when(instr(col(\"Start_Lng_str\"), \".\") > 0, length(col(\"Start_Lng_str\")) - instr(col(\"Start_Lng_str\"), \".\"))\n",
    "    .otherwise(0)\n",
    ")\n",
    "\n",
    "# Get the distribution of decimal places\n",
    "print(\"\\nStart_Lat Decimal Place Distribution (All Records):\")\n",
    "df.groupBy(\"Start_Lat_decimals\").count().orderBy(\"Start_Lat_decimals\").show(truncate=False)\n",
    "\n",
    "print(\"\\nStart_Lng Decimal Place Distribution (All Records):\")\n",
    "df.groupBy(\"Start_Lng_decimals\").count().orderBy(\"Start_Lng_decimals\").show(truncate=False)\n",
    "\n",
    "# Remove the temporary string and decimal columns if you wish\n",
    "df = df.drop(\"Start_Lat_str\", \"Start_Lng_str\", \"Start_Lat_decimals\", \"Start_Lng_decimals\")\n",
    "\n",
    "print(\"\\nStart_Lat Descriptive Statistics:\")\n",
    "df.select(col(\"Start_Lat\")).describe().show()\n",
    "\n",
    "print(\"\\nStart_Lng Descriptive Statistics:\")\n",
    "df.select(col(\"Start_Lng\")).describe().show()\n",
    "\n",
    "# Check for null values\n",
    "print(f\"\\nNull Start_Lat values: {df.filter(col('Start_Lat').isNull()).count()}\")\n",
    "print(f\"Null Start_Lng values: {df.filter(col('Start_Lng').isNull()).count()}\")\n",
    "\n",
    "# Check for values that fall outside typical US bounds (e.g., beyond 24-50 Lat, -66 to -125 Lng)\n",
    "lat_outliers = df.filter((col(\"Start_Lat\") < 24) | (col(\"Start_Lat\") > 50)).count()\n",
    "lng_outliers = df.filter((col(\"Start_Lng\") < -125) | (col(\"Start_Lng\") > -66)).count()\n",
    "\n",
    "print(f\"Start_Lat outliers (outside 24-50): {lat_outliers}\")\n",
    "print(f\"Start_Lng outliers (outside -125 to -66): {lng_outliers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4121eb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupBy('Roundabout').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84646301",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"--- Schema of DataFrame 'df' ---\")\n",
    "df.printSchema()\n",
    "\n",
    "print(\"\\n--- Distribution of 'Roundabout' column ---\")\n",
    "df.groupBy(\"Roundabout\").count().show()\n",
    "\n",
    "\n",
    "df = df.withColumn(\"Start_Lat\", col(\"Start_Lat\").cast(DoubleType()))\n",
    "df = df.withColumn(\"Start_Lng\", col(\"Start_Lng\").cast(DoubleType()))\n",
    "\n",
    "# Filter out rows where Start_Lat or Start_Lng are null after casting\n",
    "df_filtered_coords = df.filter(col(\"Start_Lat\").isNotNull() & col(\"Start_Lng\").isNotNull())\n",
    "\n",
    "# Define the precision for grouping nearby locations.\n",
    "ROUNDING_PRECISION = 4\n",
    "\n",
    "# 1. Create a \"location_group_id\" by rounding lat/long to group nearby accidents.\n",
    "df_with_location_key = df_filtered_coords.withColumn(\"rounded_lat\", round(col(\"Start_Lat\"), ROUNDING_PRECISION)) \\\n",
    "                                         .withColumn(\"rounded_lng\", round(col(\"Start_Lng\"), ROUNDING_PRECISION))\n",
    "\n",
    "# --- Debugging the potential agg issue ---\n",
    "print(\"\\n--- Schema of DataFrame 'df_with_location_key' before agg ---\")\n",
    "df_with_location_key.printSchema()\n",
    "\n",
    "# 2. Identify unique rounded locations that have *both*\n",
    "#    accidents where 'Roundabout' is TRUE AND accidents where 'Roundabout' is FALSE.\n",
    "#    This is our robust indicator of a potentially \"changed\" intersection.\n",
    "potential_changed_locations = df_with_location_key.groupBy(\"rounded_lat\", \"rounded_lng\") \\\n",
    "    .agg(\n",
    "        # Count accidents where 'Roundabout' is true\n",
    "        spark_sum(when(col(\"Roundabout\") == True, 1).otherwise(0)).alias(\"roundabout_accident_count\"),\n",
    "        # Count accidents where 'Roundabout' is false (meaning some other control type was present)\n",
    "        spark_sum(when(col(\"Roundabout\") == False, 1).otherwise(0)).alias(\"non_roundabout_accident_count\")\n",
    "    ) \\\n",
    "    .filter((col(\"roundabout_accident_count\") > 0) & (col(\"non_roundabout_accident_count\") > 0))\n",
    "\n",
    "# 3. Join back to the DataFrame to get all accidents associated with these identified locations\n",
    "changed_intersections_accidents = df_with_location_key.join(\n",
    "    potential_changed_locations.select(\"rounded_lat\", \"rounded_lng\"), # Select only the join keys\n",
    "    on=[\"rounded_lat\", \"rounded_lng\"],\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "# 4. Count the total number of accidents at these potentially \"changed\" intersections\n",
    "total_accidents_at_changed_intersections = changed_intersections_accidents.count()\n",
    "\n",
    "print(f\"\\nTotal count of accidents at potentially 'changed' intersections (based on {ROUNDING_PRECISION} decimal rounding for location matching): {total_accidents_at_changed_intersections}\")\n",
    "\n",
    "num_unique_changed_locations = potential_changed_locations.count()\n",
    "print(f\"Number of unique potentially 'changed' intersection locations identified: {num_unique_changed_locations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d288c51",
   "metadata": {},
   "source": [
    "## Description Investigation - Trucks, Intoxication\n",
    "\n",
    "We looked at the description field and found it was extremely limited. The \"Descriptions\" are more like radio announcements about which lanes are closed; no real accident info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6520f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "drunk_words = \"drunk|intoxicated|sobriety|alcohol|DUI\"\n",
    "\n",
    "drunk_intoxicated_accidents_count = df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(f\"(?i){drunk_words}\"))\n",
    ").count()\n",
    "\n",
    "print(f\"\\nTotal count of accidents involving {drunk_words} in the description: {drunk_intoxicated_accidents_count}\")\n",
    "\n",
    "print(f\"\\nSample of descriptions involving {drunk_words} (first 10):\")\n",
    "df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(f\"(?i){drunk_words}\"))\n",
    ").select(\"Description\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe43747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some lazy person didn't change their variable names, tsk tsk\n",
    "\n",
    "drunk_intoxicated_accidents_count = df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(\"(?i)truck\"))\n",
    ").count()\n",
    "\n",
    "print(f\"\\nTotal count of accidents involving trucks in the description: {drunk_intoxicated_accidents_count}\")\n",
    "\n",
    "print(\"\\nSample of descriptions involving trucks (first 10):\")\n",
    "df.filter(\n",
    "    col(\"Description\").isNotNull() &\n",
    "    (col(\"Description\").rlike(\"(?i)truck\"))\n",
    ").select(\"Description\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ca3b22",
   "metadata": {},
   "source": [
    "## DST Investigation\n",
    "\n",
    "Methods may be wrong as we found nothing close to statistically significant increase in accidents during periods after DST compared to before. (Some years even included a substantial drop in accidents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2490940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.withColumn(\"Date\", F.to_date(\"Start_Time\"))\n",
    "\n",
    "# Hardcoded DST start dates for each year\n",
    "dst_dates = {\n",
    "    2016: \"2016-03-13\",\n",
    "    2017: \"2017-03-12\",\n",
    "    2018: \"2018-03-11\",\n",
    "    2019: \"2019-03-10\",\n",
    "    2020: \"2020-03-08\",\n",
    "    2021: \"2021-03-14\",\n",
    "    2022: \"2022-03-13\",\n",
    "    2023: \"2023-03-12\",\n",
    "}\n",
    "\n",
    "# Collect results for each year\n",
    "results = []\n",
    "for year, dst_date in sorted(dst_dates.items()):  # Ensure correct year order\n",
    "    before_dst = df.filter(F.col(\"Date\").between(F.lit(dst_date) - F.expr(\"INTERVAL 6 DAY\"), F.lit(dst_date) - F.expr(\"INTERVAL 4 DAY\")))\n",
    "    after_dst = df.filter(F.col(\"Date\").between(F.lit(dst_date) + F.expr(\"INTERVAL 1 DAY\"), F.lit(dst_date) + F.expr(\"INTERVAL 3 DAY\")))\n",
    "\n",
    "    before_count = before_dst.count()\n",
    "    after_count = after_dst.count()\n",
    "\n",
    "    results.append((year, before_count, after_count))\n",
    "\n",
    "# Print results in ascending order\n",
    "for year, before, after in results:\n",
    "    print(f\"{year}: Accidents Monday-Wednesday BEFORE DST: {before}, AFTER DST: {after}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506654ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check on intervals\n",
    "\n",
    "# Example DataFrame with one test date\n",
    "test_df = spark.createDataFrame([(datetime(2023, 3, 12),)], [\"TestDate\"]).withColumn(\"TestDate\", F.col(\"TestDate\").cast(DateType()))\n",
    "\n",
    "# Applying INTERVAL shifts\n",
    "test_df = test_df.withColumn(\"Minus_6_Days\", F.col(\"TestDate\") - F.expr(\"INTERVAL 6 DAY\"))\n",
    "test_df = test_df.withColumn(\"Minus_4_Days\", F.col(\"TestDate\") - F.expr(\"INTERVAL 4 DAY\"))\n",
    "test_df = test_df.withColumn(\"Plus_1_Day\", F.col(\"TestDate\") + F.expr(\"INTERVAL 1 DAY\"))\n",
    "test_df = test_df.withColumn(\"Plus_3_Days\", F.col(\"TestDate\") + F.expr(\"INTERVAL 3 DAY\"))\n",
    "\n",
    "# Show results\n",
    "test_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feb156d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No one likes Mondays!\n",
    "\n",
    "df = df.withColumn(\"Date\", F.to_date(\"Start_Time\"))\n",
    "\n",
    "# Hardcoded Daylight Savings time dates (checked by calendar sampling)\n",
    "dst_dates = {\n",
    "    2016: \"2016-03-13\",\n",
    "    2017: \"2017-03-12\",\n",
    "    2018: \"2018-03-11\",\n",
    "    2019: \"2019-03-10\",\n",
    "    2020: \"2020-03-08\",\n",
    "    2021: \"2021-03-14\",\n",
    "    2022: \"2022-03-13\",\n",
    "    2023: \"2023-03-12\",\n",
    "}\n",
    "\n",
    "# Collect results for each year\n",
    "results = []\n",
    "for year, dst_date in sorted(dst_dates.items()):\n",
    "    monday_before = df.filter(F.col(\"Date\") == F.lit(dst_date) - F.expr(\"INTERVAL 6 DAY\"))\n",
    "    monday_after = df.filter(F.col(\"Date\") == F.lit(dst_date) + F.expr(\"INTERVAL 1 DAY\"))\n",
    "\n",
    "    before_count = monday_before.count()\n",
    "    after_count = monday_after.count()\n",
    "\n",
    "    results.append((year, before_count, after_count))\n",
    "\n",
    "# Print results in ascending order\n",
    "for year, before, after in results:\n",
    "    print(f\"{year}: Accidents on Monday BEFORE DST: {before}, AFTER DST: {after}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "collision_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
